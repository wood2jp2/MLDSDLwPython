Section 1: Getting Started
    5. MAC: Installing and Getting Started Using Anaconda and course Materials
        - Installed Anaconda 3.7 for MacOS. Used default settings for pretty much everything.
        - Went to fresh command line, and used anaconda to install tensorflow (make deep neural networks) and pydotplus (visualize decision trees)
            + 'conda install [package name]'
        - Downloaded zip file for course from sundog-learning.com/machine-learning.
            + This contains jupiter notebook files, interactive ways to experiment with Python code. You can launch and simulate files, and edit code right in your browser.
            + Running 'jupyter notebook' allows you to launch a local listing of all the files in your browser.
        
    10. Intro to Pandas: library of functions, processes tabular data (rows, columns)
        - Sci-kit Learn:  library for machine learning algos, linear regressions, SVM..?
        - SK Learn usually takes data formatted by NumPy
        - Pandas to load in/manipulate data, translate to NumPy array that gets fed into SKLearn
            + This will result in a 'dataframe'.
            + data frames have methods like:
                1. df.head(): shows you the first given number of rows
                2. df.tail(): shows you the last given number of columns
                3. df.shape: returns tuple of (rows, columns)
                4. df.size: number of columns (rows x columns)
                5. len(df): rows length
                6. df.columns: returns array of column names
                7. df.sort_values(['Column to sort by'])
                8. df['Column Name'].value_counts(): returns number of occurences for each unique value
            + extract values by column using df['Column Name'] or multiple columns with df[['Column One', 'Column Two']]

Section 2: Statistics and Probability Refresher, and Python Practise
    12. Types of Data
        - Numerical: quantifiable thing you can measure (stock prices, heights of people)
            1) Discrete: integer-based. counts of an event (how many times times did I flip heads?)
            2) Continuous: infinite number of possible values (how much rain fell in a given day, how long did it take to log out of website)
        - Categorical: qualitative. (gender, race, political party). No quantifiable values. You can assign numbers, but there's no real mathematical meaning to them.
        - Ordinal: Mix of Numerical and Categorical, essentially Categorical data that has mathematical meaning.
            + Think of a 5-star rating system for movies.

    13. Mean, Median, Mode
        - Mean: average of data set. total / number of values
            [1, 2, 3, 4] = mean is 2.5
        - Median: sort value and take physical middle of the bunch.
            + Median of [1, 2, 3, 4, 5] is 3
            + If you have an even number of numbers, then you take the average of the two middle numbers:
                 [1, 2, 22, 39] = median is 22+2/2 = 12.
            + Median is useful in data distributions because outliers do not affect the number. If you're taking average household income, the handful of billionaires do not skew this number.
        - Mode: most common number in data set.
            [0, 0, 2, 2, 3, 4, 5, 5, 5, 6, 6] = mode is 5
    
    14. ACTIVITY: Using mean, median, and mode in Python
        + numpy.random.normal(a, b, c): creates distribution of data centered around 'a', with a standard deviation of 'b', for 'c' amount of points. This is a glorified list of data.
        + numpy.append(dataset, arraOrListOfValues): adds values to dataset.
        + numpy.mean(dataset)
        + numpy.median(dataset)
        + In this activity, we first created a random data set centered around a value of $27,000 for household incomes, with a standard deviation of $15,000 and with 10,000 points of data. 
            - Calculating the mean and median results in values around $27,000.
        + Then we appended a fictitious Jeff Bezos, who made $1,000,000,000 last year. 
            - Calculating the new mean results in a huge skew, but the median stays approximately the same.

        + Moving onto mode: 
            - Using SciPy package, we calculated 500 random people's ages between 18-90 
            - from scipy import stats
            - stats.mode(dataset)
    
    15. ACTIVITY: Variation and Standard deviation
        + Standard Deviation: sigma. square root of variance. used as a way to identify outliers. Data points that lie greater than one standard deviation from the mean are considered 'unusual'. Extreme data is given with the amount of sigmas / std deviations from the mean it is.
        + Variance: how spread out is your data. It's the average of the squared differences from the mean.
            + Population vs. Sample
                - If you have a complete set of data (population), you divide the squared variances by the complete number of N.
                - If you have only a sample of the data, you divide the squared variances by the complete number of N-1.

        + Remember, we are using matplotlib.pyplot to visualize the data seen. 
            - plot.hist(a, b): a is the list of data, b is the number of 'buckets' you want visualized.
            There are also plenty of kwargs documented here: https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.hist.html
            - data.std(): returns standard deviation
            - data.var(): returns variance, or std deviation squared.
    
    16. Probability Density Function; Probability Mass Function
        + Probability Density Function: a 'normal distribution' is an example of this. Speaks to the probability of a data point falling within a given range of values.
        + Probability Mass Function: this is for when you're dealing with discrete data, where a data point has only a limited amount of values it can be.

    17. Common Data distributions
        + Uniform distribution: There's a flat constant probability of a value happening within a given range.
            - numpy.random.uniform(a, b, c): a is min, b is max, c is amount of values
        + Normal Gaussian: looks essentially like a bell curve, the data normalizes around a central value that is most likely of occurring and probability decreases as the values stray away in either direction of that center.
            from scipy.stats import norm
            import matplotlib.pyplot as plt

            x = numpy.arange(min, max, step)
            plt.plot(x, norm.pdf(x))
        + Exponential PDF (Probability Density Function) / "Power Law": exponentially decreases after a certain value. Steep slope down
            from scipy.stats import expon

            x = numpy.arange(min, max, step)
            plt.plot(x, expon.pdf(x))
        + Binomial Probability Mass Function
            from scipy.stats import binom

            n, p= 10, 0.5
            x = numpy.arange(min, max, step)
            plt.plot(x, binom.pmf(x, n, p))
        + Poisson Probability Mass Function: if you have info on average number of occurences of an event, this can help you predict the probability of another value happening at any one occurence.
            from scipy.stats import poisson

            mu = 500 // average
            x = numpy.arange(min, max, spacingOrStep)
            plt.plot(x, poisson.pmf(x, mu))

        NOTE: Remember, probability density function is used when we are working with Continuous data, while a probability mass function is used with discrete data.
    
    18.[ACTIVITY] Percentiles and Moments
        + Percentiles: In a data set, what's the point at which X% of the values are less than that value?
            - example: income distribution.
            numpy.percentiles(listOfValues, percentile)